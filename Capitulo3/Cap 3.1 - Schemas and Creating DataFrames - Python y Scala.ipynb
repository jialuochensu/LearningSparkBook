{"cells":[{"cell_type":"code","source":["#Página 45 - Key Merits and Benefits.\n#El porqué usar RDD de bajo nivel es poco eficiente y las ventajas de : expressivity, simplicity, composability, and uniformity"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f80b5a95-cc9b-4374-b426-c8baf0e88004","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#expressivity, composability"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"cfa58732-43f6-4e36-86ff-c1b164676c98","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#Low level RDD\n\n#Creamos una tupla de RDD (name, age)\ndataRDD = sc.parallelize([(\"Brooke\", 20), (\"Denny\", 31), (\"Jules\", 30), (\"TD\", 35), (\"Brooke\", 25)])\n\n#Usamos map y reduceByKey con expresiones lambda para transformar\nagesRDD = (\n    dataRDD\n    .map(lambda x: (x[0], (x[1], 1))) \n    .reduceByKey(lambda x, y: (x[0]+y[0], x[1]+y[1]))\n    .map(lambda x: (x[0], x[1][0]/x[1][1]))\n)\nagesRDD.collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b4830dee-3ca1-4e24-970a-f12514417833","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Out[4]: [('Brooke', 22.5), ('Denny', 31.0), ('TD', 35.0), ('Jules', 30.0)]"]}],"execution_count":0},{"cell_type":"code","source":["#Se puede observar que es un poco dificil de entender para algo tan simple. \n#Estariamos diciendo a Spark el CÓMO HACER(esto para spark es vacio, no estariamos comunicando nuestra intencion) y NO el QUÉ HACER."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"bf7fa734-760f-40e4-b83d-5faa10a549db","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#High level DSL, comunicando a spark QUÉ HACER (what to do?)\n#Creamos un dataFrame usando SparkSession\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import avg\nspark = (\n    SparkSession\n    .builder\n    .appName(\"AuthorAges\")\n    .getOrCreate()\n)\n\n#Creamos el dataFrame\ndata_df = spark.createDataFrame([(\"Brooke\", 20), (\"Denny\", 31), (\"Jules\", 30), (\"TD\", 35), \n                                 (\"Brooke\", 25)], [\"name\", \"age\"])\n#Operamos sobre ella\navg_df = data_df.groupBy(\"name\").agg(avg(\"age\"))\navg_df.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3e4f0af2-29d9-4325-8637-455ae5f728f9","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+------+--------+\n|  name|avg(age)|\n+------+--------+\n|Brooke|    22.5|\n| Denny|    31.0|\n| Jules|    30.0|\n|    TD|    35.0|\n+------+--------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["#uniformity, simplicity\n#Observamos que usando Scala es prácticamente idéntico y hace lo mismo."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7e9cfea0-bdcb-47fb-9599-7e2f00b041ec","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%scala\n\nimport org.apache.spark.sql.SparkSession\nimport org.apache.spark.sql.functions.avg\n\nval spark = (\n    SparkSession\n    .builder\n    .appName(\"AuthorAges\")\n    .getOrCreate()\n)\n\n//Creamos el dataFrame\nval data_df = spark.createDataFrame(Seq((\"Brooke\", 20), (\"Denny\", 31), (\"Jules\", 30), (\"TD\", 35), \n                                 (\"Brooke\", 25))).toDF(\"name\", \"age\")\n//Operamos sobre ella\nval avg_df = data_df.groupBy(\"name\").agg(avg(\"age\"))\navg_df.show()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"938a9b98-f8ba-45d5-8b0f-5e9f3a2dbc0b","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[{"name":"data_df","typeStr":"org.apache.spark.sql.DataFrame","schema":{"type":"struct","fields":[{"name":"name","type":"string","nullable":true,"metadata":{}},{"name":"age","type":"integer","nullable":false,"metadata":{}}]},"tableIdentifier":null},{"name":"avg_df","typeStr":"org.apache.spark.sql.DataFrame","schema":{"type":"struct","fields":[{"name":"name","type":"string","nullable":true,"metadata":{}},{"name":"avg(age)","type":"double","nullable":true,"metadata":{}}]},"tableIdentifier":null}],"data":"<div class=\"ansiout\">+------+--------+\n|  name|avg(age)|\n+------+--------+\n|Brooke|    22.5|\n| Denny|    31.0|\n| Jules|    30.0|\n|    TD|    35.0|\n+------+--------+\n\nimport org.apache.spark.sql.SparkSession\nimport org.apache.spark.sql.functions.avg\nspark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@609bbba7\ndata_df: org.apache.spark.sql.DataFrame = [name: string, age: int]\navg_df: org.apache.spark.sql.DataFrame = [name: string, avg(age): double]\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+------+--------+\n  name|avg(age)|\n+------+--------+\nBrooke|    22.5|\n Denny|    31.0|\n Jules|    30.0|\n    TD|    35.0|\n+------+--------+\n\nimport org.apache.spark.sql.SparkSession\nimport org.apache.spark.sql.functions.avg\nspark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@609bbba7\ndata_df: org.apache.spark.sql.DataFrame = [name: string, age: int]\navg_df: org.apache.spark.sql.DataFrame = [name: string, avg(age): double]\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e5862ef1-eb97-44cc-a9af-1295343591de","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Cap 3.1 - Schemas and Creating DataFrames - Python y Scala","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":2930660217689044}},"nbformat":4,"nbformat_minor":0}
